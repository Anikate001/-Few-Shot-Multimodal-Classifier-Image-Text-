{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('KAGGLE_dataset_fashion.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('dataset')\n",
        "\n"
      ],
      "metadata": {
        "id": "SK1COp8hgbG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir('dataset'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnO0pXTtlpKT",
        "outputId": "59bba178-d1ba-42ca-faa1-909ee8c8a8d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['myntradataset', 'images', 'styles.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('dataset/styles.csv', on_bad_lines='skip')\n",
        "print(df.sample(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTc3ofOvlwd7",
        "outputId": "bd88c5aa-5505-45bd-af2c-94c030d770b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          id gender masterCategory subCategory      articleType baseColour  \\\n",
            "16458   7430  Women        Apparel     Topwear           Tunics     Purple   \n",
            "6187   18548  Women       Footwear       Shoes     Casual Shoes      Beige   \n",
            "25969  19385    Men        Apparel   Innerwear  Innerwear Vests       Grey   \n",
            "13239  52585  Women    Accessories        Bags         Clutches      Green   \n",
            "38324  56983  Women       Footwear       Shoes            Flats       Blue   \n",
            "\n",
            "       season    year   usage                             productDisplayName  \n",
            "16458  Summer  2011.0  Casual  W Women Square Neck Short Sleeve Purple Tunic  \n",
            "6187   Winter  2015.0  Casual              Catwalk Women  Beige Casual Shoes  \n",
            "25969  Summer  2016.0  Casual     Crusoe Men Grey Xtreme Game Innerwear Vest  \n",
            "13239  Summer  2012.0  Casual               Mod'acc Women Light Green Clutch  \n",
            "38324  Winter  2012.0  Casual                          HM Women Blue Sandals  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "img_path = 'dataset/images/10001.jpg'\n",
        "Image.open(img_path).show()"
      ],
      "metadata": {
        "id": "zhzznby5mlQZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['productDisplayName'].fillna('')\n",
        "df['text'] = df['text'].str.lower()\n"
      ],
      "metadata": {
        "id": "G2B-1PP0n_0X"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_df = df.groupby('masterCategory')['id'].apply(lambda x: x.sample(min(len(x), 10))) \\\n",
        ".reset_index().merge(df, on=['masterCategory', 'id'])"
      ],
      "metadata": {
        "id": "-c6i1tkfoOBL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install peft  # for LoRA adapters[web:43][web:47]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZngf66xo2uQ",
        "outputId": "08b19076-785d-4353-8a29-9a9f7004bc49"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-o3lw7emb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-o3lw7emb\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (25.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.23.0+cu126)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=0e7b8b838f4eb81effd592050425d708f3fc133bdad344361f91779b0d027123\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zvsbklan/wheels/35/3e/df/3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.56.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.10.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywC61c9GypXZ",
        "outputId": "9e372cdc-8eb1-42f2-98a2-d6e89efe14b0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:04<00:00, 85.2MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "image_emb = model.encode_image(image)\n",
        "\n",
        "text = clip.tokenize([\"sample product text from your df\"]).to(device)\n",
        "text_emb = model.encode_text(text)\n"
      ],
      "metadata": {
        "id": "ux7pCSUvzG7n"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multimodal_emb = torch.cat([image_emb, text_emb], dim=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "xRzNLw6czSVo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = few_shot_df.sample(frac=0.8)\n",
        "val_df = few_shot_df.drop(train_df.index)\n"
      ],
      "metadata": {
        "id": "PXzSVi44zT2Q"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self, emb_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(emb_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8jd8dWFxzVM_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG2WDiDvzcE2",
        "outputId": "fedf2528-964e-43fc-8182-b80a71449fd5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.46.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.2)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.13.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.9)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.0)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.17.4)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.19.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def predict_fn(img, text):\n",
        "    img_emb = model.encode_image(preprocess(img).unsqueeze(0).to(device))\n",
        "    text_emb = model.encode_text(clip.tokenize([text]).to(device))\n",
        "    concat_emb = torch.cat([img_emb, text_emb], dim=1)\n",
        "    pred = classifier(concat_emb)\n",
        "    category = categories[torch.argmax(pred)]\n",
        "    return category\n",
        "\n",
        "gr.Interface(fn=predict_fn, inputs=[\"image\", \"text\"], outputs=\"label\").launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "4bt1OL-Nze_9",
        "outputId": "7cbc56ca-9bbe-410c-98ec-652a3e8eee90"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://af99ff671b80ee59c1.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://af99ff671b80ee59c1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_image(img):\n",
        "    if isinstance(img, np.ndarray):\n",
        "        img = Image.fromarray(img)\n",
        "    return preprocess(img).unsqueeze(0).to(device)\n"
      ],
      "metadata": {
        "id": "g59YWQ8S1IH2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "classifier.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "VR6PT1sj1K79",
        "outputId": "085de345-aae6-4bc7-937d-20453c2ddc28"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'classifier' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3162286059.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Example: If CLIP image & text embeddings size is 512 each (ViT-B/32 model)\n",
        "embedding_dim = 512 * 2  # concatenating image and text embeddings\n",
        "category_list = few_shot_df['masterCategory'].unique().tolist()\n",
        "num_classes = len(category_list)  # number of categories in your dataset\n",
        "\n",
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self, emb_dim, n_classes):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(emb_dim, n_classes)\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Instantiate classifier\n",
        "classifier = MultimodalClassifier(embedding_dim, num_classes).to(device)\n",
        "\n",
        "# If you have saved weights after training, load them:\n",
        "# classifier.load_state_dict(torch.load('path_to_saved_weights.pth'))"
      ],
      "metadata": {
        "id": "W-rAeNr-1S2m"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48fa2b47"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "* The `FashionDataset` class was successfully created to handle the loading and preprocessing of image and text data using CLIP.\n",
        "* PyTorch `DataLoader` objects were successfully created for training and validation datasets with a batch size of 32, with the training data shuffled.\n",
        "* The `CrossEntropyLoss` function was chosen as the criterion for classification, and the Adam optimizer was selected with a learning rate of 0.001 to update the classifier's parameters.\n",
        "* The training process ran for 10 epochs, showing a consistent decrease in training loss.\n",
        "* Validation loss decreased initially but then increased, while validation accuracy plateaued at 16.67% after the third epoch.\n",
        "* The final evaluation on the validation set resulted in a loss of 1.9427 and an accuracy of 16.67%.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "* The low validation accuracy suggests that the classifier is not performing well, likely due to overfitting to the small training dataset.\n",
        "* Next steps could include exploring data augmentation techniques, using a pre-trained classifier, or increasing the size of the training dataset to improve generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d912ac18"
      },
      "source": [
        "## Evaluate the trained classifier\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the final performance of the trained classifier on the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dffc1735"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the final performance of the trained classifier on the validation set by calculating the total validation loss and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a254994c",
        "outputId": "da194994-9d04-4384-9977-bb18c58e544b"
      },
      "source": [
        "model.eval()\n",
        "classifier.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "val_loss = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, texts, labels in val_dataloader:\n",
        "        images = images.to(device)\n",
        "        texts = texts.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        image_embeddings = model.encode_image(images)\n",
        "        text_embeddings = model.encode_text(texts)\n",
        "\n",
        "        concatenated_embeddings = torch.cat([image_embeddings, text_embeddings], dim=1)\n",
        "        outputs = classifier(concatenated_embeddings)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        val_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "epoch_val_loss = val_loss / len(val_dataset)\n",
        "accuracy = 100 * correct / total\n",
        "\n",
        "print(f'Final Validation Loss: {epoch_val_loss:.4f}, Final Validation Accuracy: {accuracy:.2f}%')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Validation Loss: 2.3002, Final Validation Accuracy: 16.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6027142c"
      },
      "source": [
        "## Train the classifier\n",
        "\n",
        "### Subtask:\n",
        "Write the training loop to iterate over the dataloaders, perform forward and backward passes, and update the model weights. Include evaluation on the validation set during training to monitor performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04c571f1"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the training loop including forward and backward passes, optimizer steps, and evaluation on the validation set, following steps 1-15 of the instructions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f922d02",
        "outputId": "63df3db2-2217-4d9d-a6c1-2df4966027f2"
      },
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    running_loss = 0.0\n",
        "    for images, texts, labels in train_dataloader:\n",
        "        images = images.to(device)\n",
        "        texts = texts.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            image_embeddings = model.encode_image(images)\n",
        "            text_embeddings = model.encode_text(texts)\n",
        "\n",
        "        concatenated_embeddings = torch.cat([image_embeddings, text_embeddings], dim=1)\n",
        "        outputs = classifier(concatenated_embeddings)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}')\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, texts, labels in val_dataloader:\n",
        "            images = images.to(device)\n",
        "            texts = texts.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            image_embeddings = model.encode_image(images)\n",
        "            text_embeddings = model.encode_text(texts)\n",
        "\n",
        "            concatenated_embeddings = torch.cat([image_embeddings, text_embeddings], dim=1)\n",
        "            outputs = classifier(concatenated_embeddings)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_val_loss = val_loss / len(val_dataset)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Training Loss: 0.7343\n",
            "Epoch 1/10, Validation Loss: 1.9730, Validation Accuracy: 16.67%\n",
            "Epoch 2/10, Training Loss: 0.6407\n",
            "Epoch 2/10, Validation Loss: 1.9819, Validation Accuracy: 16.67%\n",
            "Epoch 3/10, Training Loss: 0.5704\n",
            "Epoch 3/10, Validation Loss: 2.0013, Validation Accuracy: 16.67%\n",
            "Epoch 4/10, Training Loss: 0.5094\n",
            "Epoch 4/10, Validation Loss: 2.0400, Validation Accuracy: 16.67%\n",
            "Epoch 5/10, Training Loss: 0.4546\n",
            "Epoch 5/10, Validation Loss: 2.0795, Validation Accuracy: 16.67%\n",
            "Epoch 6/10, Training Loss: 0.4064\n",
            "Epoch 6/10, Validation Loss: 2.1256, Validation Accuracy: 16.67%\n",
            "Epoch 7/10, Training Loss: 0.3647\n",
            "Epoch 7/10, Validation Loss: 2.1739, Validation Accuracy: 16.67%\n",
            "Epoch 8/10, Training Loss: 0.3267\n",
            "Epoch 8/10, Validation Loss: 2.2200, Validation Accuracy: 16.67%\n",
            "Epoch 9/10, Training Loss: 0.2949\n",
            "Epoch 9/10, Validation Loss: 2.2603, Validation Accuracy: 16.67%\n",
            "Epoch 10/10, Training Loss: 0.2675\n",
            "Epoch 10/10, Validation Loss: 2.3002, Validation Accuracy: 16.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec6ccbe4"
      },
      "source": [
        "## Define loss function and optimizer\n",
        "\n",
        "### Subtask:\n",
        "Choose an appropriate loss function (e.g., CrossEntropyLoss for classification) and an optimizer (e.g., Adam)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b53fe5c1"
      },
      "source": [
        "**Reasoning**:\n",
        "Instantiate CrossEntropyLoss and the Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53ddec03"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=0.001)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f09a4c"
      },
      "source": [
        "## Create data loaders\n",
        "\n",
        "### Subtask:\n",
        "Create PyTorch DataLoaders for the training and validation datasets to efficiently batch and load the data during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b5c93b1"
      },
      "source": [
        "**Reasoning**:\n",
        "Instantiate the FashionDataset for train and validation dataframes and create DataLoaders for them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b74b32d"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = FashionDataset(train_df, 'dataset/images', model, preprocess)\n",
        "val_dataset = FashionDataset(val_df, 'dataset/images', model, preprocess)\n",
        "\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "182d4d9f"
      },
      "source": [
        "## Create a custom dataset\n",
        "\n",
        "### Subtask:\n",
        "Define a PyTorch Dataset to handle loading images and text data from the `few_shot_df`, preprocess the images and tokenize the text, and return the image embeddings, text embeddings, and corresponding labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16e2c76e"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define a PyTorch Dataset class to handle the data loading and preprocessing for training the multimodal classifier. This involves creating a custom class that inherits from `torch.utils.data.Dataset` and implementing the `__init__`, `__len__`, and `__getitem__` methods as described in the instructions. I will also import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f6c0158"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "import clip\n",
        "import torch\n",
        "\n",
        "class FashionDataset(Dataset):\n",
        "    def __init__(self, dataframe, img_dir, clip_model, preprocess_fn):\n",
        "        self.dataframe = dataframe\n",
        "        self.img_dir = img_dir\n",
        "        self.clip_model = clip_model\n",
        "        self.preprocess_fn = preprocess_fn\n",
        "        self.categories = dataframe['masterCategory'].unique().tolist()\n",
        "        self.category_to_label = {category: i for i, category in enumerate(self.categories)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.dataframe.iloc[idx]['id']\n",
        "        text = self.dataframe.iloc[idx]['productDisplayName']\n",
        "        category = self.dataframe.iloc[idx]['masterCategory']\n",
        "\n",
        "        img_path = os.path.join(self.img_dir, str(img_id) + '.jpg')\n",
        "        image = Image.open(img_path)\n",
        "        image_tensor = self.preprocess_fn(image)\n",
        "\n",
        "        text_tokens = clip.tokenize([text])[0]\n",
        "\n",
        "        label = self.category_to_label[category]\n",
        "\n",
        "        return image_tensor, text_tokens, label"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "327b9f6a"
      },
      "source": [
        "# Task\n",
        "Train a classifier using the `few_shot_df` dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "394d6488"
      },
      "source": [
        "## Create a custom dataset\n",
        "\n",
        "### Subtask:\n",
        "Define a PyTorch Dataset to handle loading images and text data from the `few_shot_df`, preprocess the images and tokenize the text, and return the image embeddings, text embeddings, and corresponding labels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6ca8a23"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define a PyTorch Dataset class to handle the data loading and preprocessing for training the multimodal classifier. This involves creating a custom class that inherits from `torch.utils.data.Dataset` and implementing the `__init__`, `__len__`, and `__getitem__` methods as described in the instructions. I will also import the necessary libraries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cf76e1e"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "import clip\n",
        "import torch\n",
        "\n",
        "class FashionDataset(Dataset):\n",
        "    def __init__(self, dataframe, img_dir, clip_model, preprocess_fn):\n",
        "        self.dataframe = dataframe\n",
        "        self.img_dir = img_dir\n",
        "        self.clip_model = clip_model\n",
        "        self.preprocess_fn = preprocess_fn\n",
        "        self.categories = dataframe['masterCategory'].unique().tolist()\n",
        "        self.category_to_label = {category: i for i, category in enumerate(self.categories)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.dataframe.iloc[idx]['id']\n",
        "        text = self.dataframe.iloc[idx]['productDisplayName']\n",
        "        category = self.dataframe.iloc[idx]['masterCategory']\n",
        "\n",
        "        img_path = os.path.join(self.img_dir, str(img_id) + '.jpg')\n",
        "        image = Image.open(img_path)\n",
        "        image_tensor = self.preprocess_fn(image)\n",
        "\n",
        "        text_tokens = clip.tokenize([text])[0]\n",
        "\n",
        "        label = self.category_to_label[category]\n",
        "\n",
        "        return image_tensor, text_tokens, label"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86fb940f"
      },
      "source": [
        "## Create data loaders\n",
        "\n",
        "### Subtask:\n",
        "Create PyTorch DataLoaders for the training and validation datasets to efficiently batch and load the data during training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f015fd45"
      },
      "source": [
        "**Reasoning**:\n",
        "Instantiate the FashionDataset for train and validation dataframes and create DataLoaders for them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06de1172"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = FashionDataset(train_df, 'dataset/images', model, preprocess)\n",
        "val_dataset = FashionDataset(val_df, 'dataset/images', model, preprocess)\n",
        "\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "728669b7"
      },
      "source": [
        "## Define loss function and optimizer\n",
        "\n",
        "### Subtask:\n",
        "Choose an appropriate loss function (e.g., CrossEntropyLoss for classification) and an optimizer (e.g., Adam).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7fede2d"
      },
      "source": [
        "**Reasoning**:\n",
        "Instantiate CrossEntropyLoss and the Adam optimizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58d81712"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=0.001)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ef7073c"
      },
      "source": [
        "## Train the classifier\n",
        "\n",
        "### Subtask:\n",
        "Write the training loop to iterate over the dataloaders, perform forward and backward passes, and update the model weights. Include evaluation on the validation set during training to monitor performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19ddd281"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the training loop including forward and backward passes, optimizer steps, and evaluation on the validation set, following steps 1-15 of the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcd62ee6",
        "outputId": "28768eeb-c048-4072-c797-06448e168c3f"
      },
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    running_loss = 0.0\n",
        "    for images, texts, labels in train_dataloader:\n",
        "        images = images.to(device)\n",
        "        texts = texts.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            image_embeddings = model.encode_image(images)\n",
        "            text_embeddings = model.encode_text(texts)\n",
        "\n",
        "        concatenated_embeddings = torch.cat([image_embeddings, text_embeddings], dim=1)\n",
        "        outputs = classifier(concatenated_embeddings)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}')\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, texts, labels in val_dataloader:\n",
        "            images = images.to(device)\n",
        "            texts = texts.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            image_embeddings = model.encode_image(images)\n",
        "            text_embeddings = model.encode_text(texts)\n",
        "\n",
        "            concatenated_embeddings = torch.cat([image_embeddings, text_embeddings], dim=1)\n",
        "            outputs = classifier(concatenated_embeddings)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_val_loss = val_loss / len(val_dataset)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%')\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Training Loss: 1.9617\n",
            "Epoch 1/10, Validation Loss: 1.9625, Validation Accuracy: 8.33%\n",
            "Epoch 2/10, Training Loss: 1.7605\n",
            "Epoch 2/10, Validation Loss: 1.9375, Validation Accuracy: 8.33%\n",
            "Epoch 3/10, Training Loss: 1.5894\n",
            "Epoch 3/10, Validation Loss: 1.9071, Validation Accuracy: 16.67%\n",
            "Epoch 4/10, Training Loss: 1.4368\n",
            "Epoch 4/10, Validation Loss: 1.8957, Validation Accuracy: 16.67%\n",
            "Epoch 5/10, Training Loss: 1.3044\n",
            "Epoch 5/10, Validation Loss: 1.8902, Validation Accuracy: 16.67%\n",
            "Epoch 6/10, Training Loss: 1.1835\n",
            "Epoch 6/10, Validation Loss: 1.8904, Validation Accuracy: 16.67%\n",
            "Epoch 7/10, Training Loss: 1.0723\n",
            "Epoch 7/10, Validation Loss: 1.8974, Validation Accuracy: 16.67%\n",
            "Epoch 8/10, Training Loss: 0.9726\n",
            "Epoch 8/10, Validation Loss: 1.9059, Validation Accuracy: 16.67%\n",
            "Epoch 9/10, Training Loss: 0.8813\n",
            "Epoch 9/10, Validation Loss: 1.9223, Validation Accuracy: 16.67%\n",
            "Epoch 10/10, Training Loss: 0.7991\n",
            "Epoch 10/10, Validation Loss: 1.9427, Validation Accuracy: 16.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5acf88ed"
      },
      "source": [
        "## Evaluate the trained classifier\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the final performance of the trained classifier on the validation set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "214e3776"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the final performance of the trained classifier on the validation set by calculating the total validation loss and accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a350416",
        "outputId": "d201ed46-1da0-4b1e-a9a9-38d0763c2042"
      },
      "source": [
        "model.eval()\n",
        "classifier.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "val_loss = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, texts, labels in val_dataloader:\n",
        "        images = images.to(device)\n",
        "        texts = texts.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        image_embeddings = model.encode_image(images)\n",
        "        text_embeddings = model.encode_text(texts)\n",
        "\n",
        "        concatenated_embeddings = torch.cat([image_embeddings, text_embeddings], dim=1)\n",
        "        outputs = classifier(concatenated_embeddings)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        val_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "epoch_val_loss = val_loss / len(val_dataset)\n",
        "accuracy = 100 * correct / total\n",
        "\n",
        "print(f'Final Validation Loss: {epoch_val_loss:.4f}, Final Validation Accuracy: {accuracy:.2f}%')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Validation Loss: 1.9427, Final Validation Accuracy: 16.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87b93931"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The `FashionDataset` class was successfully created to handle the loading and preprocessing of image and text data using CLIP.\n",
        "*   PyTorch `DataLoader` objects were successfully created for training and validation datasets with a batch size of 32, with the training data shuffled.\n",
        "*   The `CrossEntropyLoss` function was chosen as the criterion for classification, and the Adam optimizer was selected with a learning rate of 0.001 to update the classifier's parameters.\n",
        "*   The training process ran for 10 epochs, showing a consistent decrease in training loss.\n",
        "*   Validation loss decreased initially but then increased, while validation accuracy plateaued at 16.67% after the third epoch.\n",
        "*   The final evaluation on the validation set resulted in a loss of 1.9427 and an accuracy of 16.67%.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The low validation accuracy suggests that the classifier is not performing well, likely due to overfitting to the small training dataset.\n",
        "*   Next steps could include exploring data augmentation techniques, using a pre-trained classifier, or increasing the size of the training dataset to improve generalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Define your augmentations for training set\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),   # Resize images to CLIP input size\n",
        "    transforms.RandomHorizontalFlip(),  # Flip images horizontally 50% of time\n",
        "    transforms.RandomRotation(15),       # Rotate images +/- 15 degrees\n",
        "    transforms.ToTensor(),                # Convert to tensor\n",
        "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073),  # CLIP normalization means\n",
        "                         (0.26862954, 0.26130258, 0.27577711))  # CLIP normalization stds\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
        "                         (0.26862954, 0.26130258, 0.27577711))\n",
        "])\n"
      ],
      "metadata": {
        "id": "PaxDg-a-6jEj"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "    ...\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.df.iloc[idx]['image_path']).convert('RGB')\n",
        "        text = self.df.iloc[idx]['text']\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # other code to return img, text, label remains unchanged\n"
      ],
      "metadata": {
        "id": "dpXhnP456jDI"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = FashionDataset(train_df, transform=train_transforms)\n",
        "val_dataset = FashionDataset(val_df, transform=val_transforms)\n"
      ],
      "metadata": {
        "id": "R3v5USPM6qlp"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft transformers --quiet\n"
      ],
      "metadata": {
        "id": "Mc1gZD_A6qet"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, _ in model.visual.named_modules():\n",
        "    print(name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uXn0KUI7-zp",
        "outputId": "10fbb9b7-76b4-434b-eb59-b1eb4e35e59b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "conv1\n",
            "ln_pre\n",
            "transformer\n",
            "transformer.resblocks\n",
            "transformer.resblocks.0\n",
            "transformer.resblocks.0.attn\n",
            "transformer.resblocks.0.attn.out_proj\n",
            "transformer.resblocks.0.ln_1\n",
            "transformer.resblocks.0.mlp\n",
            "transformer.resblocks.0.mlp.c_fc\n",
            "transformer.resblocks.0.mlp.gelu\n",
            "transformer.resblocks.0.mlp.c_proj\n",
            "transformer.resblocks.0.ln_2\n",
            "transformer.resblocks.1\n",
            "transformer.resblocks.1.attn\n",
            "transformer.resblocks.1.attn.out_proj\n",
            "transformer.resblocks.1.ln_1\n",
            "transformer.resblocks.1.mlp\n",
            "transformer.resblocks.1.mlp.c_fc\n",
            "transformer.resblocks.1.mlp.gelu\n",
            "transformer.resblocks.1.mlp.c_proj\n",
            "transformer.resblocks.1.ln_2\n",
            "transformer.resblocks.2\n",
            "transformer.resblocks.2.attn\n",
            "transformer.resblocks.2.attn.out_proj\n",
            "transformer.resblocks.2.ln_1\n",
            "transformer.resblocks.2.mlp\n",
            "transformer.resblocks.2.mlp.c_fc\n",
            "transformer.resblocks.2.mlp.gelu\n",
            "transformer.resblocks.2.mlp.c_proj\n",
            "transformer.resblocks.2.ln_2\n",
            "transformer.resblocks.3\n",
            "transformer.resblocks.3.attn\n",
            "transformer.resblocks.3.attn.out_proj\n",
            "transformer.resblocks.3.ln_1\n",
            "transformer.resblocks.3.mlp\n",
            "transformer.resblocks.3.mlp.c_fc\n",
            "transformer.resblocks.3.mlp.gelu\n",
            "transformer.resblocks.3.mlp.c_proj\n",
            "transformer.resblocks.3.ln_2\n",
            "transformer.resblocks.4\n",
            "transformer.resblocks.4.attn\n",
            "transformer.resblocks.4.attn.out_proj\n",
            "transformer.resblocks.4.ln_1\n",
            "transformer.resblocks.4.mlp\n",
            "transformer.resblocks.4.mlp.c_fc\n",
            "transformer.resblocks.4.mlp.gelu\n",
            "transformer.resblocks.4.mlp.c_proj\n",
            "transformer.resblocks.4.ln_2\n",
            "transformer.resblocks.5\n",
            "transformer.resblocks.5.attn\n",
            "transformer.resblocks.5.attn.out_proj\n",
            "transformer.resblocks.5.ln_1\n",
            "transformer.resblocks.5.mlp\n",
            "transformer.resblocks.5.mlp.c_fc\n",
            "transformer.resblocks.5.mlp.gelu\n",
            "transformer.resblocks.5.mlp.c_proj\n",
            "transformer.resblocks.5.ln_2\n",
            "transformer.resblocks.6\n",
            "transformer.resblocks.6.attn\n",
            "transformer.resblocks.6.attn.out_proj\n",
            "transformer.resblocks.6.ln_1\n",
            "transformer.resblocks.6.mlp\n",
            "transformer.resblocks.6.mlp.c_fc\n",
            "transformer.resblocks.6.mlp.gelu\n",
            "transformer.resblocks.6.mlp.c_proj\n",
            "transformer.resblocks.6.ln_2\n",
            "transformer.resblocks.7\n",
            "transformer.resblocks.7.attn\n",
            "transformer.resblocks.7.attn.out_proj\n",
            "transformer.resblocks.7.ln_1\n",
            "transformer.resblocks.7.mlp\n",
            "transformer.resblocks.7.mlp.c_fc\n",
            "transformer.resblocks.7.mlp.gelu\n",
            "transformer.resblocks.7.mlp.c_proj\n",
            "transformer.resblocks.7.ln_2\n",
            "transformer.resblocks.8\n",
            "transformer.resblocks.8.attn\n",
            "transformer.resblocks.8.attn.out_proj\n",
            "transformer.resblocks.8.ln_1\n",
            "transformer.resblocks.8.mlp\n",
            "transformer.resblocks.8.mlp.c_fc\n",
            "transformer.resblocks.8.mlp.gelu\n",
            "transformer.resblocks.8.mlp.c_proj\n",
            "transformer.resblocks.8.ln_2\n",
            "transformer.resblocks.9\n",
            "transformer.resblocks.9.attn\n",
            "transformer.resblocks.9.attn.out_proj\n",
            "transformer.resblocks.9.ln_1\n",
            "transformer.resblocks.9.mlp\n",
            "transformer.resblocks.9.mlp.c_fc\n",
            "transformer.resblocks.9.mlp.gelu\n",
            "transformer.resblocks.9.mlp.c_proj\n",
            "transformer.resblocks.9.ln_2\n",
            "transformer.resblocks.10\n",
            "transformer.resblocks.10.attn\n",
            "transformer.resblocks.10.attn.out_proj\n",
            "transformer.resblocks.10.ln_1\n",
            "transformer.resblocks.10.mlp\n",
            "transformer.resblocks.10.mlp.c_fc\n",
            "transformer.resblocks.10.mlp.gelu\n",
            "transformer.resblocks.10.mlp.c_proj\n",
            "transformer.resblocks.10.ln_2\n",
            "transformer.resblocks.11\n",
            "transformer.resblocks.11.attn\n",
            "transformer.resblocks.11.attn.out_proj\n",
            "transformer.resblocks.11.ln_1\n",
            "transformer.resblocks.11.mlp\n",
            "transformer.resblocks.11.mlp.c_fc\n",
            "transformer.resblocks.11.mlp.gelu\n",
            "transformer.resblocks.11.mlp.c_proj\n",
            "transformer.resblocks.11.ln_2\n",
            "ln_post\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.visual.named_modules():\n",
        "    print(name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz1un3c_7-wO",
        "outputId": "e96cacf2-9188-4e57-964c-343ace3bd8f4"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "conv1\n",
            "ln_pre\n",
            "transformer\n",
            "transformer.resblocks\n",
            "transformer.resblocks.0\n",
            "transformer.resblocks.0.attn\n",
            "transformer.resblocks.0.attn.out_proj\n",
            "transformer.resblocks.0.ln_1\n",
            "transformer.resblocks.0.mlp\n",
            "transformer.resblocks.0.mlp.c_fc\n",
            "transformer.resblocks.0.mlp.gelu\n",
            "transformer.resblocks.0.mlp.c_proj\n",
            "transformer.resblocks.0.ln_2\n",
            "transformer.resblocks.1\n",
            "transformer.resblocks.1.attn\n",
            "transformer.resblocks.1.attn.out_proj\n",
            "transformer.resblocks.1.ln_1\n",
            "transformer.resblocks.1.mlp\n",
            "transformer.resblocks.1.mlp.c_fc\n",
            "transformer.resblocks.1.mlp.gelu\n",
            "transformer.resblocks.1.mlp.c_proj\n",
            "transformer.resblocks.1.ln_2\n",
            "transformer.resblocks.2\n",
            "transformer.resblocks.2.attn\n",
            "transformer.resblocks.2.attn.out_proj\n",
            "transformer.resblocks.2.ln_1\n",
            "transformer.resblocks.2.mlp\n",
            "transformer.resblocks.2.mlp.c_fc\n",
            "transformer.resblocks.2.mlp.gelu\n",
            "transformer.resblocks.2.mlp.c_proj\n",
            "transformer.resblocks.2.ln_2\n",
            "transformer.resblocks.3\n",
            "transformer.resblocks.3.attn\n",
            "transformer.resblocks.3.attn.out_proj\n",
            "transformer.resblocks.3.ln_1\n",
            "transformer.resblocks.3.mlp\n",
            "transformer.resblocks.3.mlp.c_fc\n",
            "transformer.resblocks.3.mlp.gelu\n",
            "transformer.resblocks.3.mlp.c_proj\n",
            "transformer.resblocks.3.ln_2\n",
            "transformer.resblocks.4\n",
            "transformer.resblocks.4.attn\n",
            "transformer.resblocks.4.attn.out_proj\n",
            "transformer.resblocks.4.ln_1\n",
            "transformer.resblocks.4.mlp\n",
            "transformer.resblocks.4.mlp.c_fc\n",
            "transformer.resblocks.4.mlp.gelu\n",
            "transformer.resblocks.4.mlp.c_proj\n",
            "transformer.resblocks.4.ln_2\n",
            "transformer.resblocks.5\n",
            "transformer.resblocks.5.attn\n",
            "transformer.resblocks.5.attn.out_proj\n",
            "transformer.resblocks.5.ln_1\n",
            "transformer.resblocks.5.mlp\n",
            "transformer.resblocks.5.mlp.c_fc\n",
            "transformer.resblocks.5.mlp.gelu\n",
            "transformer.resblocks.5.mlp.c_proj\n",
            "transformer.resblocks.5.ln_2\n",
            "transformer.resblocks.6\n",
            "transformer.resblocks.6.attn\n",
            "transformer.resblocks.6.attn.out_proj\n",
            "transformer.resblocks.6.ln_1\n",
            "transformer.resblocks.6.mlp\n",
            "transformer.resblocks.6.mlp.c_fc\n",
            "transformer.resblocks.6.mlp.gelu\n",
            "transformer.resblocks.6.mlp.c_proj\n",
            "transformer.resblocks.6.ln_2\n",
            "transformer.resblocks.7\n",
            "transformer.resblocks.7.attn\n",
            "transformer.resblocks.7.attn.out_proj\n",
            "transformer.resblocks.7.ln_1\n",
            "transformer.resblocks.7.mlp\n",
            "transformer.resblocks.7.mlp.c_fc\n",
            "transformer.resblocks.7.mlp.gelu\n",
            "transformer.resblocks.7.mlp.c_proj\n",
            "transformer.resblocks.7.ln_2\n",
            "transformer.resblocks.8\n",
            "transformer.resblocks.8.attn\n",
            "transformer.resblocks.8.attn.out_proj\n",
            "transformer.resblocks.8.ln_1\n",
            "transformer.resblocks.8.mlp\n",
            "transformer.resblocks.8.mlp.c_fc\n",
            "transformer.resblocks.8.mlp.gelu\n",
            "transformer.resblocks.8.mlp.c_proj\n",
            "transformer.resblocks.8.ln_2\n",
            "transformer.resblocks.9\n",
            "transformer.resblocks.9.attn\n",
            "transformer.resblocks.9.attn.out_proj\n",
            "transformer.resblocks.9.ln_1\n",
            "transformer.resblocks.9.mlp\n",
            "transformer.resblocks.9.mlp.c_fc\n",
            "transformer.resblocks.9.mlp.gelu\n",
            "transformer.resblocks.9.mlp.c_proj\n",
            "transformer.resblocks.9.ln_2\n",
            "transformer.resblocks.10\n",
            "transformer.resblocks.10.attn\n",
            "transformer.resblocks.10.attn.out_proj\n",
            "transformer.resblocks.10.ln_1\n",
            "transformer.resblocks.10.mlp\n",
            "transformer.resblocks.10.mlp.c_fc\n",
            "transformer.resblocks.10.mlp.gelu\n",
            "transformer.resblocks.10.mlp.c_proj\n",
            "transformer.resblocks.10.ln_2\n",
            "transformer.resblocks.11\n",
            "transformer.resblocks.11.attn\n",
            "transformer.resblocks.11.attn.out_proj\n",
            "transformer.resblocks.11.ln_1\n",
            "transformer.resblocks.11.mlp\n",
            "transformer.resblocks.11.mlp.c_fc\n",
            "transformer.resblocks.11.mlp.gelu\n",
            "transformer.resblocks.11.mlp.c_proj\n",
            "transformer.resblocks.11.ln_2\n",
            "ln_post\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze the CLIP visual encoder\n",
        "for param in model.visual.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Use model.visual as is in your classifier without LoRA\n",
        "clip_model_lora = model.visual\n"
      ],
      "metadata": {
        "id": "HNEZshYd7-uT"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.visual.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Your classifier architecture remains the same\n"
      ],
      "metadata": {
        "id": "vN0nZqm37-sc"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze CLIP visual encoder parameters\n",
        "for param in model.visual.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Define your Multimodal classifier normally with model.visual as fixed encoder\n",
        "# Training happens for only classifier parameters\n",
        "\n",
        "# Here is an example of forward pass usage:\n",
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self, clip_visual, text_embed_dim, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.clip_visual = clip_visual\n",
        "        self.text_embed_dim = text_embed_dim\n",
        "        self.image_embed_dim = clip_visual.output_dim\n",
        "\n",
        "        self.classifier_head = nn.Sequential(\n",
        "            nn.Linear(self.image_embed_dim + text_embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, text_emb):\n",
        "        with torch.no_grad():\n",
        "            img_emb = self.clip_visual(image)\n",
        "        combined_emb = torch.cat([img_emb, text_emb], dim=1)\n",
        "        return self.classifier_head(combined_emb)\n",
        "\n",
        "# Initialize classifier\n",
        "classifier = MultimodalClassifier(model.visual, 512, 512, len(category_list)).to(device)\n"
      ],
      "metadata": {
        "id": "erknKaHX7-qi"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze CLIP vision encoder to prevent training its weights\n",
        "for param in model.visual.parameters():\n",
        "    param.requires_grad = False\n"
      ],
      "metadata": {
        "id": "GNrBxhD-7-ok"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self, clip_visual, text_embed_dim, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.clip_visual = clip_visual\n",
        "        self.text_embed_dim = text_embed_dim\n",
        "        self.image_embed_dim = clip_visual.output_dim  # Make sure this attribute exists\n",
        "\n",
        "        self.classifier_head = nn.Sequential(\n",
        "            nn.Linear(self.image_embed_dim + text_embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, text_emb):\n",
        "        with torch.no_grad():\n",
        "            img_emb = self.clip_visual(image)\n",
        "        combined_emb = torch.cat([img_emb, text_emb], dim=1)\n",
        "        return self.classifier_head(combined_emb)\n",
        "\n",
        "\n",
        "hidden_dim = 512\n",
        "text_embed_dim = 512\n",
        "num_classes = len(category_list)  # category_list should be defined\n",
        "\n",
        "classifier = MultimodalClassifier(model.visual, text_embed_dim, hidden_dim, num_classes).to(device)\n"
      ],
      "metadata": {
        "id": "HzcxAY_g9MU-"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self, clip_visual, text_embed_dim, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.clip_visual = clip_visual\n",
        "        self.text_embed_dim = text_embed_dim\n",
        "        self.image_embed_dim = clip_visual.output_dim  # Ensure this attribute exists\n",
        "\n",
        "        self.classifier_head = nn.Sequential(\n",
        "            nn.Linear(self.image_embed_dim + text_embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, text_emb):\n",
        "        with torch.no_grad():\n",
        "            img_emb = self.clip_visual(image)\n",
        "        combined_emb = torch.cat([img_emb, text_emb], dim=1)\n",
        "        return self.classifier_head(combined_emb)\n",
        "\n",
        "\n",
        "hidden_dim = 512\n",
        "text_embed_dim = 512  # CLIP ViT-B/32 text embedding size\n",
        "num_classes = len(category_list)  # Make sure category_list is defined earlier\n",
        "\n",
        "classifier = MultimodalClassifier(model.visual, text_embed_dim, hidden_dim, num_classes).to(device)\n"
      ],
      "metadata": {
        "id": "EqtoHJ829Xeo"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, text_tokens, labels in train_dataloader:\n",
        "    images = images.to(device)\n",
        "    text_tokens = text_tokens.to(device)\n",
        "    labels = labels.to(device)\n",
        "    # Use in training loop next"
      ],
      "metadata": {
        "id": "Ynvqi_eR9dRD"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=1e-3)\n",
        "epochs = 10  # You can increase later\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    classifier.train()\n",
        "    running_loss = 0\n",
        "\n",
        "    for images, text_tokens, labels in train_dataloader:\n",
        "        images, text_tokens, labels = images.to(device), text_tokens.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Encode text with frozen CLIP encoder\n",
        "        with torch.no_grad():\n",
        "            text_emb = model.encode_text(text_tokens)\n",
        "\n",
        "        outputs = classifier(images, text_emb)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_dataloader):.4f}\")\n",
        "\n",
        "    # Optionally, add validation evaluation here"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJRYbpM89dPM",
        "outputId": "9d0c0776-93f5-434f-cd93-17c068498997"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.9254\n",
            "Epoch 2, Loss: 1.3973\n",
            "Epoch 3, Loss: 1.0621\n",
            "Epoch 4, Loss: 0.7526\n",
            "Epoch 5, Loss: 0.4853\n",
            "Epoch 6, Loss: 0.3411\n",
            "Epoch 7, Loss: 0.2251\n",
            "Epoch 8, Loss: 0.1559\n",
            "Epoch 9, Loss: 0.1015\n",
            "Epoch 10, Loss: 0.0755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(classifier.state_dict(), 'multimodal_classifier_frozen_clip.pth')\n"
      ],
      "metadata": {
        "id": "Dfs-iZ0z9dMy"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.load_state_dict(torch.load('multimodal_classifier_frozen_clip.pth'))\n",
        "classifier.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FpPlxmW9dKR",
        "outputId": "55265706-8c92-4dd0-b2b2-fafb4a6be750"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultimodalClassifier(\n",
              "  (clip_visual): VisionTransformer(\n",
              "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (transformer): Transformer(\n",
              "      (resblocks): Sequential(\n",
              "        (0): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (classifier_head): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.2, inplace=False)\n",
              "    (3): Linear(in_features=512, out_features=7, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Optimizer for classifier only\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=1e-3)\n",
        "\n",
        "# Number of training epochs\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    classifier.train()\n",
        "    running_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for images, text_tokens, labels in train_dataloader:\n",
        "        images = images.to(device)\n",
        "        text_tokens = text_tokens.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Encode text with frozen CLIP text encoder\n",
        "        with torch.no_grad():\n",
        "            text_emb = model.encode_text(text_tokens)\n",
        "\n",
        "        # Forward pass through classifier\n",
        "        outputs = classifier(images, text_emb)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy on batch\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataloader)\n",
        "    epoch_acc = total_correct / total_samples\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6rnmeJz-fuI",
        "outputId": "4cdf6f14-2145-43df-e8b8-7e8515afdf72"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Loss: 0.0554 - Accuracy: 100.00%\n",
            "Epoch 2/10 - Loss: 0.0570 - Accuracy: 100.00%\n",
            "Epoch 3/10 - Loss: 0.0231 - Accuracy: 100.00%\n",
            "Epoch 4/10 - Loss: 0.0357 - Accuracy: 100.00%\n",
            "Epoch 5/10 - Loss: 0.0128 - Accuracy: 100.00%\n",
            "Epoch 6/10 - Loss: 0.0061 - Accuracy: 100.00%\n",
            "Epoch 7/10 - Loss: 0.0088 - Accuracy: 100.00%\n",
            "Epoch 8/10 - Loss: 0.0073 - Accuracy: 100.00%\n",
            "Epoch 9/10 - Loss: 0.0044 - Accuracy: 100.00%\n",
            "Epoch 10/10 - Loss: 0.0025 - Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, classifier, val_loader):\n",
        "    classifier.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    running_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, text_tokens, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            text_tokens = text_tokens.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            text_emb = model.encode_text(text_tokens)\n",
        "            outputs = classifier(images, text_emb)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    val_loss = running_loss / len(val_loader)\n",
        "    val_acc = total_correct / total_samples\n",
        "\n",
        "    print(f\"Validation - Loss: {val_loss:.4f}, Accuracy: {val_acc*100:.2f}%\")\n",
        "    return val_loss, val_acc\n"
      ],
      "metadata": {
        "id": "itJ3s_eM-fpm"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(classifier.state_dict(), 'multimodal_classifier_frozen_clip.pth')\n"
      ],
      "metadata": {
        "id": "PpGxq5pr-fnQ"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.load_state_dict(torch.load('multimodal_classifier_frozen_clip.pth'))\n",
        "classifier.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A3TuYFB-flO",
        "outputId": "716c6e55-8404-428a-c29d-b542ca6a0b91"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultimodalClassifier(\n",
              "  (clip_visual): VisionTransformer(\n",
              "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (transformer): Transformer(\n",
              "      (resblocks): Sequential(\n",
              "        (0): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (classifier_head): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.2, inplace=False)\n",
              "    (3): Linear(in_features=512, out_features=7, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model weights into your classifier\n",
        "classifier.load_state_dict(torch.load('multimodal_classifier_frozen_clip.pth'))\n",
        "\n",
        "# Put the model in evaluation mode for inference\n",
        "classifier.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdug1JRX-fjV",
        "outputId": "8488cf27-067f-4a41-b862-988db8c45cc4"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultimodalClassifier(\n",
              "  (clip_visual): VisionTransformer(\n",
              "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (transformer): Transformer(\n",
              "      (resblocks): Sequential(\n",
              "        (0): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (classifier_head): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.2, inplace=False)\n",
              "    (3): Linear(in_features=512, out_features=7, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G8v9y2hc-feO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
